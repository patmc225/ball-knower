name: Daily Player Scrape

on:
  schedule:
    # Run at 09:00 UTC daily (1 AM PST / 4 AM EST)
    - cron: '0 9 * * *'
  workflow_dispatch: # Allow manual trigger

permissions:
  contents: write

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    timeout-minutes: 360 # Allow up to 6 hours as the scraper is rate-limited

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scraper/requirements.txt

    - name: Run Scraper
      run: |
        cd scraper
        python run_scraper.py

    - name: Check for changes
      id: check_changes
      run: |
        if [[ -n $(git status --porcelain) ]]; then
          echo "changes_exist=true" >> $GITHUB_OUTPUT
        else
          echo "changes_exist=false" >> $GITHUB_OUTPUT
        fi

    - name: Commit and Push changes
      if: steps.check_changes.outputs.changes_exist == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Stage specific files
        git add ballknower/public/backend/players_new.json
        git add ballknower/public/backend/metadata.json
        git add scraper/metadata.json
        
        # Commit if there are changes staged
        if ! git diff --staged --quiet; then
          git commit -m "Automated: Update player data [skip ci]"
          git push
        else
          echo "No changes to tracked files."
        fi

